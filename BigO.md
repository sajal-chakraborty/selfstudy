Iâ€™ll teach Big-O notation step-by-step, from first principles, with Java/DSA thinking, and always connect it to interview usage.

1ï¸âƒ£ Why Big-O Exists (Before Definition)

In DSA, we donâ€™t ask:

â€œHow fast is this code on my laptop?â€

We ask:

How does this algorithm scale when input size grows?

Big-O helps us answer:

What happens when n = 10 â†’ 1,000 â†’ 1,000,000

Will it survive production traffic

Will it pass time limits in interviews

ğŸ‘‰ Big-O measures growth rate, not exact time.

2ï¸âƒ£ What Big-O Actually Means

Big-O notation describes the worst-case growth of:

Time Complexity â†’ number of operations

Space Complexity â†’ extra memory used

Formally:

Big-O expresses how runtime or memory grows as input size n â†’ infinity

3ï¸âƒ£ Time Complexity â€“ Core Intuition
Example 1: Constant Time â€“ O(1)
int getFirst(int[] arr) {
    return arr[0];
}


Input size = n

Operations = always 1

âœ… O(1) â€“ Best possible

Real examples

Accessing array index

HashMap get (average case)

Stack push/pop

Example 2: Linear Time â€“ O(n)
void printAll(int[] arr) {
    for (int x : arr) {
        System.out.println(x);
    }
}


n elements â†’ n operations

ğŸ“ˆ Growth is linear
âœ… O(n)

Example 3: Nested Loops â€“ O(nÂ²)
void printPairs(int[] arr) {
    for (int i : arr) {
        for (int j : arr) {
            System.out.println(i + "," + j);
        }
    }
}


Outer loop â†’ n

Inner loop â†’ n

Total â†’ n Ã— n

âŒ O(nÂ²) â€“ Be careful in interviews

4ï¸âƒ£ Very Important Rule: Drop Constants ğŸš¨
for (int i = 0; i < n; i++) { }      // n
for (int i = 0; i < n; i++) { }      // n


Total = 2n

But in Big-O:

O(2n) â†’ O(n)

Why?

Constants donâ€™t change growth behavior

5ï¸âƒ£ Common Time Complexities (Must Memorize)
Big-O	Name	Typical Example
O(1)	Constant	Array access
O(log n)	Logarithmic	Binary search
O(n)	Linear	Loop
O(n log n)	Linearithmic	Merge sort
O(nÂ²)	Quadratic	Nested loops
O(2â¿)	Exponential	Subset recursion
O(n!)	Factorial	Permutations
6ï¸âƒ£ Logarithmic Time â€“ O(log n) (Very Important)
int binarySearch(int[] arr, int target) {
    int l = 0, r = arr.length - 1;
    while (l <= r) {
        int mid = (l + r) / 2;
        if (arr[mid] == target) return mid;
        if (arr[mid] < target) l = mid + 1;
        else r = mid - 1;
    }
    return -1;
}


Each step:

Cuts input in half

n â†’ n/2 â†’ n/4 â†’ n/8

ğŸ“‰ Thatâ€™s logâ‚‚(n)
âœ… O(log n)

7ï¸âƒ£ Recursion & Big-O (Key DSA Skill)
Example: Fibonacci (Bad Version)
int fib(int n) {
    if (n <= 1) return n;
    return fib(n-1) + fib(n-2);
}


Each call creates two more calls

Tree grows exponentially

âŒ Time: O(2â¿)
âŒ Space: O(n) (call stack)

8ï¸âƒ£ Space Complexity â€“ Often Ignored, Very Important

Space complexity counts:

Extra arrays

Objects

Recursion stack

Example 1: Constant Space â€“ O(1)
int sum(int a, int b) {
    return a + b;
}


No extra memory â†’ O(1)

Example 2: Linear Space â€“ O(n)
int[] copy(int[] arr) {
    int[] temp = new int[arr.length];
    return temp;
}


Extra array of size n â†’ O(n)

Example 3: Recursion Stack
void recurse(int n) {
    if (n == 0) return;
    recurse(n - 1);
}


Depth = n

Stack frames = n

ğŸ“¦ Space = O(n)

9ï¸âƒ£ Time vs Space Trade-off (Interview Favorite)

Example: Fibonacci optimized

Approach	Time	Space
Recursive	O(2â¿)	O(n)
DP (array)	O(n)	O(n)
Iterative	O(n)	O(1)

Interviewers love when you mention trade-offs explicitly.

ğŸ”Ÿ How Interviewers Expect You to Answer

âŒ Bad:

â€œIt is fastâ€

âœ… Good:

â€œTime complexity is O(n log n) due to divide and conquer, and space complexity is O(n) because of auxiliary array usage.â€

1ï¸âƒ£1ï¸âƒ£ Mental Checklist for Any Code

Ask yourself:

How many loops?

Are loops nested or sequential?

Does input reduce (log n)?

Is recursion creating multiple branches?

Any extra memory?

1ï¸âƒ£2ï¸âƒ£ Quick Visual Growth Intuition
ğŸ” Summary (Memorize This)

Big-O = growth, not exact time

Worst case matters

Ignore constants

Nested loops multiply

Recursion = tree depth Ã— branches

Space includes call stack
